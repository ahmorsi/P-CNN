\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{schuldt2004recognizing}
\citation{laptev2008learning}
\citation{wang2013action}
\citation{cheron2015p}
\citation{krizhevsky2012imagenet}
\citation{lecun1998gradient}
\citation{chatfield2014return}
\citation{he2015deep}
\citation{wang2013action}
\citation{perronnin2010improving}
\citation{simonyan2014two}
\citation{yue2015beyond}
\citation{ni2014multiple}
\citation{zhou2015interaction}
\citation{jhuang2013towards}
\citation{jhuang2013towards}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related work}{1}{section.2}}
\newlabel{related-work}{{2}{1}{\hskip -1em.~Related work}{section.2}{}}
\citation{Alpher02}
\citation{Alpher03}
\citation{Alpher04}
\citation{Alpher03}
\citation{Alpher03}
\citation{Alpher03}
\citation{Alpher02}
\citation{Authors13}
\citation{Alpher02}
\citation{Alpher03}
\citation{Authors13}
\citation{jhuang2013towards}
\citation{kuehne2011hmdb}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces P-CNN features. From left to right: Input video. Human pose and corresponding human body parts for one frame of the video. Patches of appearance (RGB) and optical flow for human body parts. One RGB and one flow CNN descriptor $f^{p}_{t}$ is extracted per frame \textit  {t} and per part \textit  {p} (an example is shown for the human body part \textit  {right hand}). Static frame descriptors $f^{p}_{t}$ are aggregated over time using \textit  {min} and \textit  {max} to obtain the video descriptor $v^{p}_{stat}$. Similarly, temporal differences of $f^{p}_{t}$ are aggregated to $v^{p}_{dyn}$. Video descriptors are normalized and concatenated over parts \textit  {p} and aggregation schemes into appearance features $v_{app}$ and flow features $v_{of}$. The final P-CNN feature is the concatenation of $v_{app}$ and $v_{of}$. }}{2}{figure.1}}
\newlabel{fig:pcnn}{{1}{2}{P-CNN features. From left to right: Input video. Human pose and corresponding human body parts for one frame of the video. Patches of appearance (RGB) and optical flow for human body parts. One RGB and one flow CNN descriptor $f^{p}_{t}$ is extracted per frame \textit {t} and per part \textit {p} (an example is shown for the human body part \textit {right hand}). Static frame descriptors $f^{p}_{t}$ are aggregated over time using \textit {min} and \textit {max} to obtain the video descriptor $v^{p}_{stat}$. Similarly, temporal differences of $f^{p}_{t}$ are aggregated to $v^{p}_{dyn}$. Video descriptors are normalized and concatenated over parts \textit {p} and aggregation schemes into appearance features $v_{app}$ and flow features $v_{of}$. The final P-CNN feature is the concatenation of $v_{app}$ and $v_{of}$}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}P-CNN: Pose-based CNN features}{2}{section.3}}
\newlabel{pcnn-features}{{3}{2}{\hskip -1em.~P-CNN: Pose-based CNN features}{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}IPCNN: Improved P-CNN}{2}{section.4}}
\newlabel{ipcnn}{{4}{2}{\hskip -1em.~IPCNN: Improved P-CNN}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Datasets}{2}{section.5}}
\newlabel{dataset}{{5}{2}{\hskip -1em.~Datasets}{section.5}{}}
\@writefile{toc}{\contentsline {paragraph}{JHMDB}{2}{section*.1}}
\citation{Authors13}
\bibstyle{ieee}
\bibdata{egbib}
\bibcite{chatfield2014return}{1}
\bibcite{cheron2015p}{2}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Examples from JHMDB dataset}}{3}{figure.2}}
\newlabel{fig:JHMDB}{{2}{3}{Examples from JHMDB dataset}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}\hskip -1em.\nobreakspace  {}Experimental results}{3}{section.6}}
\newlabel{results}{{6}{3}{\hskip -1em.~Experimental results}{section.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Results. Ours is better.}}{3}{table.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}\hskip -1em.\nobreakspace  {}Footnotes}{3}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}\hskip -1em.\nobreakspace  {}References}{3}{subsection.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}\hskip -1em.\nobreakspace  {}Illustrations, graphs, and photographs}{3}{subsection.6.3}}
\bibcite{he2015deep}{3}
\bibcite{jhuang2013towards}{4}
\bibcite{krizhevsky2012imagenet}{5}
\bibcite{kuehne2011hmdb}{6}
\bibcite{laptev2008learning}{7}
\bibcite{lecun1998gradient}{8}
\bibcite{ni2014multiple}{9}
\bibcite{perronnin2010improving}{10}
\bibcite{schuldt2004recognizing}{11}
\bibcite{simonyan2014two}{12}
\bibcite{wang2013action}{13}
\bibcite{yue2015beyond}{14}
\bibcite{zhou2015interaction}{15}
