\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{schuldt2004recognizing}
\citation{laptev2008learning}
\citation{wang2013action}
\citation{cheron2015p}
\citation{krizhevsky2012imagenet}
\citation{lecun1998gradient}
\citation{chatfield2014return}
\citation{he2015deep}
\citation{wang2013action}
\citation{perronnin2010improving}
\citation{simonyan2014two}
\citation{yue2015beyond}
\citation{ni2014multiple}
\citation{zhou2015interaction}
\citation{jhuang2013towards}
\citation{jhuang2013towards}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related work}{1}{section.2}}
\newlabel{related-work}{{2}{1}{\hskip -1em.~Related work}{section.2}{}}
\citation{cheron2015p}
\citation{brox2004high}
\citation{gkioxari2015finding}
\citation{krizhevsky2012imagenet}
\citation{chatfield2014return}
\citation{gkioxari2015finding}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces P-CNN features. From left to right: Input video. Human pose and corresponding human body parts for one frame of the video. Patches of appearance (RGB) and optical flow for human body parts. One RGB and one flow CNN descriptor $f^{p}_{t}$ is extracted per frame \textit  {t} and per part \textit  {p} (an example is shown for the human body part \textit  {right hand}). Static frame descriptors $f^{p}_{t}$ are aggregated over time using \textit  {min} and \textit  {max} to obtain the video descriptor $v^{p}_{stat}$. Similarly, temporal differences of $f^{p}_{t}$ are aggregated to $v^{p}_{dyn}$. Video descriptors are normalized and concatenated over parts \textit  {p} and aggregation schemes into appearance features $v_{app}$ and flow features $v_{of}$. The final P-CNN feature is the concatenation of $v_{app}$ and $v_{of}$. }}{2}{figure.1}}
\newlabel{fig:pcnn}{{1}{2}{P-CNN features. From left to right: Input video. Human pose and corresponding human body parts for one frame of the video. Patches of appearance (RGB) and optical flow for human body parts. One RGB and one flow CNN descriptor $f^{p}_{t}$ is extracted per frame \textit {t} and per part \textit {p} (an example is shown for the human body part \textit {right hand}). Static frame descriptors $f^{p}_{t}$ are aggregated over time using \textit {min} and \textit {max} to obtain the video descriptor $v^{p}_{stat}$. Similarly, temporal differences of $f^{p}_{t}$ are aggregated to $v^{p}_{dyn}$. Video descriptors are normalized and concatenated over parts \textit {p} and aggregation schemes into appearance features $v_{app}$ and flow features $v_{of}$. The final P-CNN feature is the concatenation of $v_{app}$ and $v_{of}$}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}P-CNN: Pose-based CNN features}{2}{section.3}}
\newlabel{pcnn-features}{{3}{2}{\hskip -1em.~P-CNN: Pose-based CNN features}{section.3}{}}
\newlabel{eq1}{{1}{2}{\hskip -1em.~P-CNN: Pose-based CNN features}{equation.3.1}{}}
\newlabel{eq2}{{2}{2}{\hskip -1em.~P-CNN: Pose-based CNN features}{equation.3.2}{}}
\citation{he2015deep}
\citation{szegedy2015going}
\citation{cheron2015p}
\citation{cheron2015p}
\citation{cherian2014mixing}
\citation{jhuang2013towards}
\citation{jhuang2013towards}
\citation{jhuang2013towards}
\citation{kuehne2011hmdb}
\newlabel{eq3}{{3}{3}{\hskip -1em.~P-CNN: Pose-based CNN features}{equation.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}IPCNN: Improved P-CNN}{3}{section.4}}
\newlabel{ipcnn}{{4}{3}{\hskip -1em.~IPCNN: Improved P-CNN}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}P-CNN without Pose Estimation}{3}{section.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Performance of different approaches in ImageNet 2015 competition.}}{3}{figure.2}}
\newlabel{fig:imagenet}{{2}{3}{Performance of different approaches in ImageNet 2015 competition}{figure.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Impact of automatic pose estimation versus ground-truth pose (GT) for P-CNN features and HLPF\cite  {jhuang2013towards}.Results are presented for JHMDB (\% accuracy )}}{3}{table.1}}
\newlabel{table:pose}{{1}{3}{Impact of automatic pose estimation versus ground-truth pose (GT) for P-CNN features and HLPF\cite {jhuang2013towards}.Results are presented for JHMDB (\% accuracy )}{table.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}\hskip -1em.\nobreakspace  {}Datasets}{3}{section.6}}
\newlabel{dataset}{{6}{3}{\hskip -1em.~Datasets}{section.6}{}}
\@writefile{toc}{\contentsline {paragraph}{JHMDB}{3}{section*.1}}
\citation{cheron2015p}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Example network architectures for ImageNet. Left: the VGG-19 model (19.6 billion FLOPs) as a reference. Middle: a plain network with 34 parameter layers (3.6 billion FLOPs). Right: a residual network with 34 parameter layers (3.6 billion FLOPs). The dotted shortcuts increase dimensions. }}{4}{figure.3}}
\newlabel{fig:resnet}{{3}{4}{Example network architectures for ImageNet. Left: the VGG-19 model (19.6 billion FLOPs) as a reference. Middle: a plain network with 34 parameter layers (3.6 billion FLOPs). Right: a residual network with 34 parameter layers (3.6 billion FLOPs). The dotted shortcuts increase dimensions}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Illustration of the cropping approaches without Pose information. \textbf  {\textit  {NoPose}} : crops of an image subdivided in quarters and a center crop regardless the human bounding box. \textbf  {\textit  {NoPose-BB}}: similar to \textit  {NoPose} but within the human bounding box.}}{4}{figure.4}}
\newlabel{fig:nopse}{{4}{4}{Illustration of the cropping approaches without Pose information. \textbf {\textit {NoPose}} : crops of an image subdivided in quarters and a center crop regardless the human bounding box. \textbf {\textit {NoPose-BB}}: similar to \textit {NoPose} but within the human bounding box}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Examples from JHMDB dataset}}{4}{figure.5}}
\newlabel{fig:JHMDB}{{5}{4}{Examples from JHMDB dataset}{figure.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Performance of appearance-based (App) and flow-based (OF) P-CNN features. Results are obtained with maxaggregation for JHMDB-GT (\% accuracy)}}{4}{table.2}}
\newlabel{table:diffparts}{{2}{4}{Performance of appearance-based (App) and flow-based (OF) P-CNN features. Results are obtained with maxaggregation for JHMDB-GT (\% accuracy)}{table.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}\hskip -1em.\nobreakspace  {}Experimental results}{4}{section.7}}
\newlabel{results}{{7}{4}{\hskip -1em.~Experimental results}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}\hskip -1em.\nobreakspace  {}Performance of human part features}{4}{subsection.7.1}}
\newlabel{diffParts}{{7.1}{4}{\hskip -1em.~Performance of human part features}{subsection.7.1}{}}
\bibstyle{ieee}
\bibdata{egbib}
\bibcite{brox2004high}{1}
\bibcite{chatfield2014return}{2}
\bibcite{cherian2014mixing}{3}
\bibcite{cheron2015p}{4}
\bibcite{gkioxari2015finding}{5}
\bibcite{he2015deep}{6}
\bibcite{jhuang2013towards}{7}
\bibcite{krizhevsky2012imagenet}{8}
\bibcite{kuehne2011hmdb}{9}
\bibcite{laptev2008learning}{10}
\bibcite{lecun1998gradient}{11}
\bibcite{ni2014multiple}{12}
\bibcite{perronnin2010improving}{13}
\bibcite{schuldt2004recognizing}{14}
\bibcite{simonyan2014two}{15}
\bibcite{szegedy2015going}{16}
\bibcite{wang2013action}{17}
\bibcite{yue2015beyond}{18}
\bibcite{zhou2015interaction}{19}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces P-CNN vs IP-CNN on JHMDB(\% accuracy)}}{5}{table.3}}
\newlabel{table:resnet}{{3}{5}{P-CNN vs IP-CNN on JHMDB(\% accuracy)}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}\hskip -1em.\nobreakspace  {}Performance of different pre-trained CNN models (Vgg-f vs ResNet)}{5}{subsection.7.2}}
\newlabel{resnetVsvgg}{{7.2}{5}{\hskip -1em.~Performance of different pre-trained CNN models (Vgg-f vs ResNet)}{table.3}{}}
